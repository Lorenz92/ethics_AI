# Life and death decisions of autonomous vehicles

* Our results challenge this idea, revealing that this apparent preference for inequality is driven by the specific ‘trolley-type’ paradigm used by the MME. Multiple studies with a revised paradigm reveal that people overwhelmingly want autonomous vehicles to treat different human lives equally in life and death situations, ignoring gender, age and status—a preference consistent with a general desire for equality2–4 . The large-scale adoption of autonomous vehicles raises ethical challenges because autonomous vehicles may sometimes have to decide between killing one person or another
* However, the MME also concludes that people want autonomous vehicles to make decisions about who to kill on the basis of personal features, including physical fitness, age, status and gender (for example, saving women and killing men). This conclusion contradicts well-documented ethical preferences for equal treatment across demographic features and identities, a preference enshrined in the US Constitution, the United Nations Universal Declaration of Human Rights and in the Ethical Guideline 9 of the German Ethics Code for Automated and Connected Driving
* We suggest that the MME finds preferences for inequality across lives because its methodology is relatively insensitive to preferences for equality. The MME uses trolley-type dilemmas that force people to choose between killing one person (or set of people) versus killing another person (or set of people). Because this paradigm assumes inequality (for example, should we program AVs to kill men or women?), it has difficulties revealing whether people prefer equality (for example, should we program AVs to ignore gender?).
* the forced inequality condition closely match the global effects of the MME. Beyond the general value of replication10, this validates our paradigm: although we used a different sample and a simpler method, we obtained the same results as the MME
* The equality allowed condition was similar to the forced inequality condition, but with the addition of a third option, (3) treat the lives of groups A and B equally (for example, treat the lives of children and elderly people equally). As Fig.1 shows, people overwhelmingly selected this option when it was available, revealing that they want autonomous vehicles to treat people equally. For example, when forced to choose between men and women, 87.7% chose to save women, but 97.9% of people actually preferred to treat both groups equally.
* Ignoring personal features is also more consistent with the current technical capacities of AVs.
* The only substantial departure from study 1 was lawfulness: 53.1% of people preferred to spare law abiders over law breakers.
* Consistent with our predictions, 89.9% of participants chose the structuralfeatures-only car, once again expressing a desire for AVs that ignore personal features in ethical dilemmas.
* What might happen if the MME forced people to choose between black and white people? Aggregating people’s decisions could reveal a racial bias13, but this would not mean that people want to share the road with racist autonomous vehicles.
* The same logic applies to the features that were included in the MME. Do people truly want to live in a world with sexist, ageist and classist self-driving cars? This thought experiment further suggests that aggregating across forced-choice preferences may not accurately reveal how people want autonomous vehicles to be programmed to act when human lives are at stake

## Notes

* Limite delle macchine di identificare un oggetto principale causa di "killing situation".
* Idea di vedere l'uguaglianza e assumere che non tutti gli scontri possono essere fatali e scegliere sulla base delle probabilità di sopravvivenza.
